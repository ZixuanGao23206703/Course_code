sensitivity <- confusion_matrix$byClass['Sensitivity']
specificity <- confusion_matrix$byClass['Specificity']
f1_score <- confusion_matrix$byClass['F1']
# Calculate AUC
roc_result <- roc(response = test_response, predictor = as.numeric(predictions), levels = rev(levels(test_response)))
auc <- auc(roc_result)
# Append to the performance data frame
performance <- rbind(performance, data.frame(Model = model_name,
Accuracy = accuracy,
AUC = auc,
F1score = f1_score,
Sensitivity = sensitivity,
Specificity = specificity))
}
# Print the performance metrics
print(performance)
# Optional: Select and print the best model based on Test Accuracy or AUC
best_model <- performance[which.max(performance$Accuracy),]
cat("The best model on test set is", best_model$Model,
"with an Accuracy of", best_model$Accuracy,
"and AUC of", best_model$AUC,
"and F1 Score of", best_model$F1score,
", Sensitivity of", best_model$Sensitivity,
", and Specificity of", best_model$Specificity, "\n")
library(ggplot2)
performance$Model <- factor(performance$Model, levels = performance$Model)
# Draw a bar plot
ggplot(performance, aes(x = Model, y = Accuracy, fill = Model)) +
geom_bar(stat = "identity", show.legend = FALSE) +
theme_minimal() +
labs(title = "Comparison of Model Accuracy", x = "Model", y = "Accuracy") +
geom_text(aes(label = sprintf("%.2f%%", Accuracy * 100)), vjust = -0.5, size = 3.5) +  # Display accuracy above the bars
coord_flip()    # Flip the coordinate axes
library(ggplot2)
library(tidyr)  # For pivot_longer
# 将数据框转换为长格式
performance_long <- pivot_longer(performance,
cols = c(Accuracy, F1score, Sensitivity, Specificity),
names_to = "Metric",
values_to = "Value")
# 将模型名称转换为因子，以保证图中的顺序
performance_long$Model <- factor(performance_long$Model, levels = unique(performance_long$Model))
# 绘制图形
ggplot(performance_long, aes(x = Model, y = Value, fill = Metric)) +
geom_bar(stat = "identity", position = position_dodge()) +
theme_minimal() +
labs(title = "Comparison of Model Metrics", x = "Model", y = "Metric Value") +
facet_wrap(~ Metric, scales = "free_y") +
geom_text(aes(label = sprintf("%.2f", Value)), position = position_dodge(0.9), vjust = -0.5, size = 3) +
coord_flip()
# 初始化存储模型性能指标的数据框
model_performance <- data.frame(Model = character(), Threshold = numeric(), TPR = numeric(), FPR = numeric())
# 使用每个模型的最佳参数集进行评估和计算ROC曲线相关统计
for(model_name in names(models)) {
model <- models[[model_name]]$finalModel
best_params <- models[[model_name]]$bestTune
# 预测概率，确保使用最佳参数
probability_predictions <- predict(model, newdata = test_predictors, type = "response")
# 计算ROC曲线
roc_curve <- roc(response = test_response, predictor = probability_predictions[, "biden"])  # 确保使用正确的预测列
# 存储ROC曲线数据
roc_data[[model_name]] <- roc_curve
# 获取最佳阈值下的TPR和FPR，基于Youden's index
optimal_idx <- which.max(roc_curve$sensitivities + roc_curve$specificities - 1)
optimal_threshold := roc_curve$thresholds[optimal_idx]
optimal_sensitivity := roc_curve$sensitivities[optimal_idx]
optimal_specificity := roc_curve$specificities[optimal_idx]
# 将每个模型的最佳TPR和FPR添加到数据框中
model_performance <- rbind(model_performance, data.frame(
Model = model_name,
Threshold = optimal_threshold,
TPR = optimal_sensitivity,
FPR = 1 - optimal_specificity
))
}
# Choose the best model (based on AUC or accuracy)
best_model_name <- performance[which.max(performance$AUC), "Model"]
# Extract the best model
best_model <- models[[best_model_name]]
# Make predictions on the test data using the best model
predictions <- predict(best_model, newdata = test_predictors)
predicted_probabilities <- predict(best_model, newdata = test_predictors, type = "prob")
# Calculate performance metrics
conf_matrix <- confusionMatrix(predictions, test_response)
accuracy <- conf_matrix$overall['Accuracy']
auc_value <- roc(response = test_response, predictor = predicted_probabilities[, "trump"])$auc
f1_score <- conf_matrix$byClass['F1']
# Output performance metrics
cat(sprintf("The best model (%s) has an Accuracy of %.2f, AUC of %.2f, and F1 Score of %.2f.\n",
best_model_name, accuracy, auc_value, f1_score))
# Visualize the predicted probability distribution
ggplot(data.frame(Actual = test_response, Predicted_Probability = predicted_probabilities[, "trump"]), aes(x = Actual, y = Predicted_Probability)) +
geom_boxplot(aes(color = Actual)) +
labs(title = sprintf("Predicted Probability Distribution for Trump (Model: %s)", best_model_name),
x = "Actual Category",
y = "Predicted Probability for Trump") +
theme_minimal()
# Choose the best model (based on AUC or accuracy)
best_model_name <- performance[which.max(performance$AUC), "Model"]
# Extract the best model
best_model <- models[[best_model_name]]
# Make predictions on the test data using the best model
predictions <- predict(best_model, newdata = test_predictors)
predicted_probabilities <- predict(best_model, newdata = test_predictors, type = "prob")
# Calculate performance metrics
conf_matrix <- confusionMatrix(predictions, test_response)
accuracy <- conf_matrix$overall['Accuracy']
auc_value <- roc(response = test_response, predictor = predicted_probabilities[, "trump"])$auc
f1_score <- conf_matrix$byClass['F1']
# Output performance metrics
cat(sprintf("The best model (%s) has an Accuracy of %.2f, AUC of %.2f, and F1 Score of %.2f.\n",
best_model_name, accuracy, auc_value, f1_score))
# Visualize the predicted probability distribution
ggplot(data.frame(Actual = test_response, Predicted_Probability = predicted_probabilities[, "trump"]), aes(x = Actual, y = Predicted_Probability)) +
geom_boxplot(aes(color = Actual)) +
labs(title = sprintf("Predicted Probability Distribution for Trump (Model: %s)", best_model_name),
x = "Actual Category",
y = "Predicted Probability for Trump") +
theme_minimal()
load("data_assignment_2.Rdata")
data <- elections
# str(data)
sum(is.na(data))
data_new <- data[, -c(1, 2)] # Remove 'state' and 'county' columns
data_new$winner <- factor(data_new$winner, levels = c("trump","biden")) # Organize factor data for response variable
# Standardize numerical columns
num_cols <- sapply(data_new, is.numeric)  # Determine which columns are numeric
data_new[, num_cols] <- scale(data_new[, num_cols])  # Standardize only the numeric columns
set.seed(23206703)
train_indices <- sample(1:nrow(data_new), 0.8 * nrow(data_new))
train_data <- data_new[train_indices, ]
test_data <- data_new[-train_indices, ]
library(caret)
library(pROC)  # for AUC calculation
# Predict and evaluate each model's performance on the test set
train_predictors <- train_data[,-ncol(train_data)]
train_response <- train_data$winner
# Initialize a list to store models
models <- list()
performance <- data.frame(Model = character(), Accuracy = numeric(), AUC = numeric())
# Set trainControl object
control <- trainControl(
method = "cv",
number = 10,
savePredictions = "final",
classProbs = TRUE,
summaryFunction = twoClassSummary
)
library(caret)
library(pROC)  # for AUC calculation
# Predict and evaluate each model's performance on the test set
train_predictors <- train_data[,-ncol(train_data)]
train_response <- train_data$winner
# Initialize a list to store models
models <- list()
performance <- data.frame(Model = character(), Accuracy = numeric(), AUC = numeric())
# Set trainControl object
control <- trainControl(
method = "cv",
number = 10,
savePredictions = "final",
classProbs = TRUE,
summaryFunction = twoClassSummary
)
# model tuning and model fitting
# Logistic Regression
set.seed(23206703)
glmGrid <- expand.grid(alpha = c(0, 0.5, 1),  # L1和L2正则化的混合比例
lambda = 10^seq(-4, -1, length = 10))  # 正则化强度
suppressWarnings({
models$Logistic <- train(train_predictors, train_response, method = 'glm',
family = 'binomial', trControl = trainControl(method = "cv", number = 10))
})
# Support Vector Machine (SVM)
set.seed(23206703)
svmGrid <- expand.grid(.sigma = seq(0.01, 0.1, length = 5), .C = seq(1, 100, length = 5))
models$SVM <- train(train_predictors, train_response, method='svmRadial',
trControl=control, tuneGrid=svmGrid, preProcess = c("center", "scale"))
# Random Forest
set.seed(23206703)
rfGrid <- expand.grid(.mtry = c(2, sqrt(ncol(train_predictors)), ncol(train_predictors)/3))
control <- trainControl(method="cv", number=10)
models$RandomForest <- train(train_predictors, train_response, method='rf',
trControl=control, tuneGrid=rfGrid)
# K-Nearest Neighbors (KNN)
set.seed(23206703)
knnGrid <- expand.grid(.k = seq(1, 21, by = 2))  # 调整k的值，通常选奇数以避免平票
models$KNN <- train(train_predictors, train_response, method='knn',
trControl=control, tuneGrid=knnGrid)
print(models)
str(data)
load("data_assignment_2.Rdata")
data <- elections
summary(data)
sum(is.na(data))
data_new <- data[, -c(1, 2)] # Remove 'state' and 'county' columns
data_new$winner <- factor(data_new$winner, levels = c("trump","biden")) # Organize factor data for response variable
# Standardize numerical columns
num_cols <- sapply(data_new, is.numeric)  # Determine which columns are numeric
data_new[, num_cols] <- scale(data_new[, num_cols])  # Standardize only the numeric columns
ggplot(data, aes(x=median_age_2019, y=median_household_income_2019, color=winner)) + geom_point() + facet_wrap(~winner)
library(ggplot2)
numeric_features <- sapply(data, is.numeric)
data_numeric <- data[, numeric_features]
melt_data <- reshape2::melt(data_numeric)
ggplot(melt_data, aes(value)) + geom_histogram(bins=30) + facet_wrap(~variable, scales = 'free_x')
correlations <- cor(data_numeric)
ggplot2::ggcorrplot(correlations)
ggplot(data, aes(x=winner, y=median_household_income_2019, fill=winner)) + geom_violin()
load("data_assignment_2.Rdata")
data <- elections
summary(data)
sum(is.na(data))
data_new <- data[, -c(1, 2)] # Remove 'state' and 'county' columns
data_new$winner <- factor(data_new$winner, levels = c("trump","biden")) # Organize factor data for response variable
# Standardize numerical columns
num_cols <- sapply(data_new, is.numeric)  # Determine which columns are numeric
data_new[, num_cols] <- scale(data_new[, num_cols])  # Standardize only the numeric columns
ggplot(data, aes(x=median_age_2019, y=median_household_income_2019, color=winner)) + geom_point() + facet_wrap(~winner)
ggplot(data, aes(x=winner, y=median_household_income_2019, fill=winner)) + geom_violin()
ggplot(data, aes(x=winner, y=c(white_2019,black_2019,asian_2019,hispanic_2019), fill=winner)) + geom_violin()
# 将数据集转换为长格式
long_data <- pivot_longer(data,
cols = c(white_2019, black_2019, asian_2019, hispanic_2019),
names_to = "ethnicity",
values_to = "percentage")
# 使用ggplot2创建小提琴图
ggplot(long_data, aes(x=ethnicity, y=percentage, fill=winner)) +
geom_violin() +
facet_wrap(~winner) + # 分面显示不同的胜利者
labs(title = "Ethnicity Percentage by Election Winner",
x = "Ethnicity",
y = "Percentage") +
theme_minimal()
load("data_assignment_2.Rdata")
data <- elections
summary(data)
sum(is.na(data))
data_new <- data[, -c(1, 2)] # Remove 'state' and 'county' columns
data_new$winner <- factor(data_new$winner, levels = c("trump","biden")) # Organize factor data for response variable
# Standardize numerical columns
num_cols <- sapply(data_new, is.numeric)  # Determine which columns are numeric
data_new[, num_cols] <- scale(data_new[, num_cols])  # Standardize only the numeric columns
library(tidyr)
library(ggplot2)
ggplot(data, aes(x=median_age_2019, y=median_household_income_2019, color=winner)) + geom_point() + facet_wrap(~winner)
ggplot(data, aes(x=winner, y=median_household_income_2019, fill=winner)) + geom_violin()
# 将数据集转换为长格式
long_data <- pivot_longer(data,
cols = c(white_2019, black_2019, asian_2019, hispanic_2019),
names_to = "ethnicity",
values_to = "percentage")
# 使用ggplot2创建小提琴图
ggplot(long_data, aes(x=ethnicity, y=percentage, fill=winner)) +
geom_violin() +
facet_wrap(~winner) + # 分面显示不同的胜利者
labs(title = "Ethnicity Percentage by Election Winner",
x = "Ethnicity",
y = "Percentage") +
theme_minimal()
load("data_assignment_2.Rdata")
data <- elections
summary(data)
sum(is.na(data))
data_new <- data[, -c(1, 2)] # Remove 'state' and 'county' columns
data_new$winner <- factor(data_new$winner, levels = c("trump","biden")) # Organize factor data for response variable
# Standardize numerical columns
num_cols <- sapply(data_new, is.numeric)  # Determine which columns are numeric
data_new[, num_cols] <- scale(data_new[, num_cols])  # Standardize only the numeric columns
library(tidyr)
library(ggplot2)
ggplot(data, aes(x=median_age_2019, y=median_household_income_2019, color=winner)) + geom_point() + facet_wrap(~winner)
long_data <- pivot_longer(data,
cols = c(white_2019, black_2019, asian_2019, hispanic_2019),
names_to = "ethnicity",
values_to = "percentage")
ggplot(long_data, aes(x=ethnicity, y=percentage, fill=winner)) +
geom_violin() +
facet_wrap(~winner) +
labs(title = "Ethnicity Percentage by Election Winner",
x = "Ethnicity",
y = "Percentage") +
theme_minimal()
load("data_assignment_2.Rdata")
data <- elections
# summary(data)
sum(is.na(data))
data_new <- data[, -c(1, 2)] # Remove 'state' and 'county' columns
data_new$winner <- factor(data_new$winner, levels = c("trump","biden")) # Organize factor data for response variable
# Standardize numerical columns
num_cols <- sapply(data_new, is.numeric)  # Determine which columns are numeric
data_new[, num_cols] <- scale(data_new[, num_cols])  # Standardize only the numeric columns
set.seed(23206703)
train_indices <- sample(1:nrow(data_new), 0.8 * nrow(data_new))
train_data <- data_new[train_indices, ]
test_data <- data_new[-train_indices, ]
set.seed(23206703)
train_indices <- sample(1:nrow(data_new), 0.8 * nrow(data_new))
train_data <- data_new[train_indices, ]
test_data <- data_new[-train_indices, ]
library(caret)
library(pROC)  # for AUC calculation
# Predict and evaluate each model's performance on the test set
train_predictors <- train_data[,-ncol(train_data)]
train_response <- train_data$winner
# Initialize a list to store models
models <- list()
performance <- data.frame(Model = character(), Accuracy = numeric(), AUC = numeric())
# Set trainControl object
control <- trainControl(
method = "cv",
number = 10,
savePredictions = "final",
classProbs = TRUE,
summaryFunction = twoClassSummary
)
# Predict and evaluate each model's performance on the test set
train_predictors <- train_data[,-ncol(train_data)]
train_response <- train_data$winner
# Initialize a list to store models
models <- list()
performance <- data.frame(Model = character(), Accuracy = numeric(), AUC = numeric())
# Set trainControl object
control <- trainControl(
method = "cv",
number = 10,
savePredictions = "final",
classProbs = TRUE,
summaryFunction = twoClassSummary
)
# model tuning and model fitting
# Logistic Regression
set.seed(23206703)
glmGrid <- expand.grid(alpha = c(0, 0.5, 1),  # L1和L2正则化的混合比例
lambda = 10^seq(-4, -1, length = 10))  # 正则化强度
suppressWarnings({
models$Logistic <- train(train_predictors, train_response, method = 'glm',
family = 'binomial',
trControl = trainControl(method = "cv", number = 10))
})
# Support Vector Machine (SVM)
set.seed(23206703)
svmGrid <- expand.grid(.sigma = seq(0.01, 0.1, length = 5), .C = seq(1, 100, length = 5))
models$SVM <- train(train_predictors, train_response, method='svmRadial',
trControl=control, tuneGrid=svmGrid, preProcess = c("center", "scale"))
# Random Forest
set.seed(23206703)
rfGrid <- expand.grid(.mtry = c(2, sqrt(ncol(train_predictors)), ncol(train_predictors)/3))
control <- trainControl(method="cv", number=10)
models$RandomForest <- train(train_predictors, train_response, method='rf',
trControl=control, tuneGrid=rfGrid)
# K-Nearest Neighbors (KNN)
set.seed(23206703)
knnGrid <- expand.grid(.k = seq(1, 21, by = 2))  # 调整k的值，通常选奇数以避免平票
models$KNN <- train(train_predictors, train_response, method='knn',
trControl=control, tuneGrid=knnGrid)
for (model_name in names(models)) {
model <- models[[model_name]]
best_params <- model$bestTune
print(paste("Best parameters for", model_name, ":", toString(best_params)))
}
# Choose the best model (based on AUC or accuracy)
best_model_name <- performance[which.max(performance$AUC), "Model"]
# Extract the best model
best_model <- models[[best_model_name]]
load("data_assignment_2.Rdata")
data <- elections
# summary(data)
sum(is.na(data))
data_new <- data[, -c(1, 2)] # Remove 'state' and 'county' columns
data_new$winner <- factor(data_new$winner, levels = c("trump","biden")) # Organize factor data for response variable
# Standardize numerical columns
num_cols <- sapply(data_new, is.numeric)  # Determine which columns are numeric
data_new[, num_cols] <- scale(data_new[, num_cols])  # Standardize only the numeric columns
# separate training data and test data
set.seed(23206703)
train_indices <- sample(1:nrow(data_new), 0.8 * nrow(data_new))
train_data <- data_new[train_indices, ]
test_data <- data_new[-train_indices, ]
library(caret)
library(pROC)  # for AUC calculation
# Predict and evaluate each model's performance on the test set
train_predictors <- train_data[,-ncol(train_data)]
train_response <- train_data$winner
# Initialize a list to store models
models <- list()
performance <- data.frame(Model = character(), Accuracy = numeric(), AUC = numeric())
# Set trainControl object
control <- trainControl(
method = "cv",
number = 10,
savePredictions = "final",
classProbs = TRUE,
summaryFunction = twoClassSummary
)
# model tuning and model fitting
# Logistic Regression
set.seed(23206703)
glmGrid <- expand.grid(alpha = c(0, 0.5, 1),
lambda = 10^seq(-4, -1, length = 10))
suppressWarnings({
models$Logistic <- train(train_predictors, train_response, method = 'glm',
family = 'binomial',
trControl = trainControl(method = "cv", number = 10))
})
# Support Vector Machine (SVM)
set.seed(23206703)
svmGrid <- expand.grid(.sigma = seq(0.01, 0.1, length = 5), .C = seq(1, 100, length = 5))
models$SVM <- train(train_predictors, train_response, method='svmRadial',
trControl=control, tuneGrid=svmGrid, preProcess = c("center", "scale"))
# Random Forest
set.seed(23206703)
rfGrid <- expand.grid(.mtry = c(2, sqrt(ncol(train_predictors)), ncol(train_predictors)/3))
control <- trainControl(method="cv", number=10)
models$RandomForest <- train(train_predictors, train_response, method='rf',
trControl=control, tuneGrid=rfGrid)
# K-Nearest Neighbors (KNN)
set.seed(23206703)
knnGrid <- expand.grid(.k = seq(1, 21, by = 2))
models$KNN <- train(train_predictors, train_response, method='knn',
trControl=control, tuneGrid=knnGrid)
for (model_name in names(models)) {
model <- models[[model_name]]
best_params <- model$bestTune
print(paste("Best parameters for", model_name, ":", toString(best_params)))
}
# Predict and evaluate each model's performance on the test set
test_predictors <- test_data[,-ncol(test_data)]
test_response <- test_data$winner
# Add ROC library for AUC calculation
library(pROC)
# Initialize a data frame to store all performance metrics
performance <- data.frame(Model = character(),
Accuracy = numeric(),
AUC = numeric(),
F1score = numeric(),
Sensitivity = numeric(),
Specificity = numeric())
# Loop through each model, make predictions, and calculate performance metrics
for(model_name in names(models)) {
# Predict using the current model
predictions <- predict(models[[model_name]], newdata = test_predictors)
# Calculate confusion matrix
confusion_matrix <- confusionMatrix(predictions, test_response)
accuracy <- confusion_matrix$overall['Accuracy']
sensitivity <- confusion_matrix$byClass['Sensitivity']
specificity <- confusion_matrix$byClass['Specificity']
f1_score <- confusion_matrix$byClass['F1']
# Calculate AUC
roc_result <- roc(response = test_response, predictor = as.numeric(predictions),
levels = rev(levels(test_response)))
auc <- auc(roc_result)
# Append to the performance data frame
performance <- rbind(performance, data.frame(Model = model_name,
Accuracy = accuracy,
AUC = auc,
F1score = f1_score,
Sensitivity = sensitivity,
Specificity = specificity))
}
# Print the performance metrics
print(performance)
# Optional: Select and print the best model based on Test Accuracy or AUC
best_model <- performance[which.max(performance$Accuracy),]
cat("The best model on test set is", best_model$Model,
"with an Accuracy of", best_model$Accuracy,
"and AUC of", best_model$AUC,
"and F1 Score of", best_model$F1score,
", Sensitivity of", best_model$Sensitivity,
", and Specificity of", best_model$Specificity, "\n")
library(ggplot2)
library(tidyr)  # For pivot_longer
performance_long <- pivot_longer(performance,
cols = c(Accuracy, F1score, Sensitivity, Specificity),
names_to = "Metric",
values_to = "Value")
performance_long$Model <- factor(performance_long$Model, levels = unique(performance_long$Model))
ggplot(performance_long, aes(x = Model, y = Value, fill = Metric)) +
geom_bar(stat = "identity", position = position_dodge()) +
theme_minimal() +
labs(title = "Comparison of Model Metrics", x = "Model", y = "Metric Value") +
facet_wrap(~ Metric, scales = "free_y") +
geom_text(aes(label = sprintf("%.2f", Value)), position = position_dodge(0.9),
vjust = -0.5, size = 3) +
coord_flip()
# Choose the best model (based on AUC or accuracy)
best_model_name <- performance[which.max(performance$AUC), "Model"]
# Extract the best model
best_model <- models[[best_model_name]]
# Make predictions on the test data using the best model
predictions <- predict(best_model, newdata = test_predictors)
predicted_probabilities <- predict(best_model, newdata = test_predictors, type = "prob")
# Calculate performance metrics
conf_matrix <- confusionMatrix(predictions, test_response)
accuracy <- conf_matrix$overall['Accuracy']
auc_value <- roc(response = test_response, predictor = predicted_probabilities[, "trump"])$auc
f1_score <- conf_matrix$byClass['F1']
# Output performance metrics
cat(sprintf("The best model (%s) has an Accuracy of %.2f, AUC of %.2f, and F1 Score of %.2f.\n",
best_model_name, accuracy, auc_value, f1_score))
# Visualize the predicted probability distribution
ggplot(data.frame(Actual = test_response, Predicted_Probability =
predicted_probabilities[, "trump"]),
aes(x = Actual, y = Predicted_Probability)) +
geom_boxplot(aes(color = Actual)) +
labs(title = sprintf("Predicted Probability Distribution for Trump (Model: %s)",
best_model_name),
x = "Actual Category",
y = "Predicted Probability for Trump") +
theme_minimal()
(1.76-0.62)^2+(-1.3-0.06)^2+(2.39-1.63)^2+(-0.73-0.62)^2+(0.83-0.06)^2+(0.85-1.63)^2+(0.82-0.62)^2+(0.64-0.06)^2+(1.66-1.63)^2
(2.41 +0.11)^2+(0.66-0.92)^2+(0.41-0.02)^2+(-2.63 +0.11)^2+(1.18-0.92)^2+(-0.36-0.02)^2
